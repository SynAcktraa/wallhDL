#!/bin/bash


#An inefficient program to download anime wallpapers from wallhaven.cc


declare -ig head_flag=0
declare -ig tail_flag=0
declare -ig getcnt
declare -ag query
declare -ag link_dump
declare -rg base_url="https://wallhaven.cc/search?q="
dferror="FlagError: can't use both flags at the same time."
sferror="FlagError: can't use same flags more than once."

help() { 
  echo "Usage: ${0##*/} [ -q \"query\" ] [ -p/-t <int> ]" 1>&2 
}

force_exit() { 
  help
  exit 1
}

while getopts ":q:p:t:h" flags; do
    case "${flags}" in
    
       q) 
          query="${OPTARG}"
          ;;
       p) 
          head_flag+=1
          if (( tail_flag==1 )); then
            echo $dferror
            force_exit
          else 
            if (( head_flag>1 )); then
              echo $sferror
              force_exit
            fi
          fi
          getcnt=${OPTARG}
          ;;
       t) 
          tail_flag+=1
          if (( head_flag==1 )); then
            echo $dferror
            force_exit
          else 
            if (( tail_flag>1 )); then
              echo $sferror
              force_exit
            fi
          fi
          getcnt=${OPTARG}
          ;;
       *) 
          help
          printf "Cli Option:- \n\
        -q = specify the query string\
          \n\t-p = specify n to download n no.s of jpeg files from top\
          \n\t-t = specify n to download n no.s of jpeg files from bottom\
          \n\t-h = show help section\n"
          exit 1
          ;;
     esac
done

getf() {
  printf "1) top\n2) bottom\n: " 
  read -r sel
  if (( sel == 1 )); then 
    head_flag=1
  else 
    tail_flag=1 
  fi
}

[ -z "$*" ] && printf "Query: " && read -r query && getf && printf "Jpeg files count: " && read -r getcnt

enc_query=$(echo "$query" | sed 's_ _%20_g')

validate=$(curl -s  "${base_url}${enc_query}" | grep -o "There's nothing here...")
[[ ! -z "$validate" ]] && echo "Sorry dude, something went wrong." && exit 1

t_pages=$(curl -s "$base_url$enc_query&categories=110&purity=100&sorting=relevance&order=desc&page=2" \
   | grep -Eo 'thumb-listing-page-num">2</span> / [0-9]{1,}'| awk 'END {print (NR > 0 && NF > 0) ? $3 : 1}')

last_page_file_count=$(curl -s "${base_url}${enc_query}&categories=110&purity=100&sorting=relevance&order=desc&page=${t_pages}" \
	| grep -Eo "https:\/\/th.wallhaven.cc\/small\/\w{2}\/\w{6}.jpg" | wc -l)

wpcnt=$(( ${last_page_file_count} + $(expr 24 \* $(( ${t_pages} - 1 ))) ))
printf "Found %d jpeg files for the query: %s\n" "$wpcnt" "$query"

if [[ $getcnt -gt $wpcnt ]]; then
     getcnt=$wpcnt
     printf "Query out of range. Defaulting to %s\n" "$getcnt"
     l_pages=$t_pages
elif [[ $(( $getcnt % 24 )) == 0 ]]; then
     l_pages=$(( $getcnt / 24 ))
else
     l_pages=$(expr $(( $getcnt / 24 )) + 1)
fi


get_ldump() {
    
    link_dump+=$(curl -s "$base_url$enc_query&categories=110&purity=100&sorting=relevance&order=desc&page=$1" \
     | grep -Eo "https:\/\/th.wallhaven.cc\/small\/\w{2}\/\w{6}.jpg"; echo " ")
}

if (( head_flag == 1 )); then
    for (( i=1 ; i <= $l_pages ; i++ )); do
	get_ldump $i
    done

elif (( tail_flag == 1 )); then
    for (( i=$t_pages; i >= $(( $t_pages - $l_pages )); i-- )); do
	get_ldump $i
    done
fi

printf "Fetching %d jpeg image files for the query: %s\n" "$getcnt" "$query"

if (( head_flag == 1 )); then
	wget -s $(echo $link_dump | sed 's_ _\n_g' | head -n $getcnt)
else
	wget -s $(echo $link_dump | sed 's_ _\n_g' | tail -n $getcnt)

fi
rm -f *.jpg.*
